<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="How to parallelize a Variational Monte Carlo code with MPI and OpenMP">

<title>How to parallelize a Variational Monte Carlo code with MPI and OpenMP</title>







<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- If the query includes 'print-pdf', use the PDF print sheet -->
<script>
document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1 style="text-align: center;">How to parallelize a Variational Monte Carlo code with MPI and OpenMP</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>


<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>National Superconducting Cyclotron Laboratory and Department of Physics and Astronomy, Michigan State University, East Lansing, MI 48824, USA</b></center>
<center>[2] <b>Department of Physics, University of Oslo, Oslo, Norway</b></center>
<p>&nbsp;<br>
<center><h4>Spring 2015</h4></center> <!-- date -->
<p>

</section>


<section>

<h2 id="___sec0">Your background  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> You have some experience in programming but have never tried to parallelize your codes</li>
<p><li> Here I will base my examples on C/C++ using Message Passing Interface (MPI) and OpenMP.</li> 
<p><li> I will also give you some simple hints on how to run and install codes on your laptop/office PC</li>
<p><li> The programs and slides can be found at the weblink</li>
<p><li> Good text: Karniadakis and Kirby, Parallel Scientific Computing in C++ and MPI, Cambridge.</li>
</ul>
<p>

We will discuss Message passing interface (MPI) and OpenMP.
</div>


<p>

</section>


<section>

<h2><div id="___sec1"></div></h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Develop codes locally, run with some few processes and test your codes.  Do benchmarking, timing and so forth on local nodes, for example your laptop or PC.  You can install MPICH2 on  your laptop/PC.</li> 
<p><li> Test by typing <em>which mpd</em></li>
<p><li> When you are convinced that your codes run correctly, you start your production runs on available supercomputers, in our case <em>smaug</em> locally (to be discussed after Easter).</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec2">How do I run MPI on a PC/Laptop?  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Most machines at computer labs at UiO are quad-cores

<ul>
<p><li> Compile with mpicxx or mpic++ or mpif90</li>
<p><li> Set up collaboration between processes and run</li> 
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpd --ncpus=<span style="color: #B452CD">4</span> &amp;
<span style="color: #1e889b">#  run code with</span>
mpiexec -n <span style="color: #B452CD">4</span> ./nameofprog
</pre></div>
<p>
Here we declare that we will use 4 processes via the <em>-ncpus</em> option and via \( -n 4 \) when running.

<ul>
<p><li> End with <em>mpdallexit</em></li>
</ul>
</div>


</section>


<section>

<h2 id="___sec3">Can I do it on my own PC/laptop? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Of course:

<ul>
<p><li> go to the website of <a href="http://www.mcs.anl.gov/research/projects/mpich2/" target="_blank">Argonne National Lab</a></li>
<p><li> follow the instructions and install it on your own PC/laptop</li>
<p><li> Versions for Ubuntu/Linux, windows and mac</li>
<p><li> For windows, you may think of installing WUBI</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec4">What is Message Passing Interface (MPI)?  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
MPI is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C/C++ or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

<p>
MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

<p>
An MPI computation is a collection of processes communicating with messages.


</div>


</section>


<section>

<h2 id="___sec5">Going Parallel with MPI </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<b>Task parallelism</b>: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulations or numerical integration are examples of this.

<p>
MPI is a message-passing library where all the routines
have corresponding C/C++-binding
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_COMMAND_NAME
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec6">MPI is a library  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
MPI is a library specification for the message passing interface,
proposed as a standard.

<ul>
<p><li> independent of hardware;</li>
<p><li> not a language or compiler specification;</li>
<p><li> not a specific implementation or product.</li>
</ul>
<p>

A message passing standard for portability and ease-of-use. 
Designed for high performance.

<p>
Insert communication and synchronization functions where necessary.


</div>


</section>


<section>

<h2 id="___sec7">The basic ideas of parallel computing </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Pursuit of shorter computation time and larger simulation size gives rise to parallel computing.</li>
<p><li> Multiple processors are involved to solve a global problem.</li>
<p><li> The essence is to divide the entire computation evenly among collaborative processors.  Divide and conquer.</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec8">A rough classification of hardware models  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Conventional single-processor computers can be called SISD (single-instruction-single-data) machines.</li>
<p><li> SIMD (single-instruction-multiple-data) machines incorporate the idea of parallel processing, which use a large number of processing units to execute the same instruction on different data.</li>
<p><li> Modern parallel computers are so-called MIMD (multiple-instruction-multiple-data) machines and can execute different instruction streams in parallel on different data.</li>
</ul>
</div>


</section>


<section>

<h2 id="___sec9">Shared memory and distributed memory </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> One way of categorizing modern parallel computers is to look at the memory configuration.</li>
<p><li> In shared memory systems the CPUs share the same address space. Any CPU can access any data in the global memory.</li>
<p><li> In distributed memory systems each CPU has its own memory.</li>
</ul>
<p>

The CPUs are connected by some network and may exchange
messages.


</div>


<p>

</section>


<section>

<h2 id="___sec10">Different parallel programming paradigms  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> <b>Task parallelism</b>:  the work of a global problem can be divided into a number of independent tasks, which rarely need to synchronize.  Monte Carlo simulation is one example. Integration is another. However this paradigm is of limited use.</li>
<p><li> <b>Data parallelism</b>:  use of multiple threads (e.g. one thread per processor) to dissect loops over arrays etc.  This paradigm requires a single memory address space.  Communication and synchronization between processors are often hidden, thus easy to program. However, the user surrenders much control to a specialized compiler. Examples of data parallelism are compiler-based parallelization and OpenMP directives.</li> 
</ul>
</div>


</section>


<section>

<h2 id="___sec11">Different parallel programming paradigms </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> <b>Message passing</b>:  all involved processors have an independent memory address space. The user is responsible for  partitioning the data/work of a global problem and distributing the  subproblems to the processors. Collaboration between processors is achieved by explicit message passing, which is used for data transfer plus synchronization.</li>
<p><li> This paradigm is the most general one where the user has full control. Better parallel efficiency is usually achieved by explicit message passing. However, message-passing programming is more difficult.</li>
</ul>
</div>


</section>


<section>

<h2 id="___sec12">SPMD (single-program-multiple-data) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Although message-passing programming supports MIMD, it 
suffices with an SPMD (single-program-multiple-data) model, which
is flexible enough for practical cases:

<ul>
<p><li> Same executable for all the processors.</li>
<p><li> Each processor works primarily with its assigned local data.</li>
<p><li> Progression of code is allowed to differ between synchronization points.</li>
<p><li> Possible to have a master/slave model. The standard option in Monte Carlo calculations and numerical integration.</li>
</ul>
</div>


</section>


<section>

<h2 id="___sec13">Today's situation of parallel computing  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Distributed memory is the dominant hardware configuration. There is a large diversity in these machines, from  MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which are very cost-effective.</li>
<p><li> Message-passing is a mature programming paradigm and widely accepted. It often provides an efficient match to the hardware. It is primarily used for the distributed memory systems, but can also be used on shared memory systems.</li>
<p><li> Modern nodes have nowadays several cores, which makes it interesting to use both shared memory (the given node) and distributed memory (several nodes with communication). This leads often to codes which use both MPI and OpenMP.</li>
</ul>
<p>

Our lectures will focus on both MPI and OpenMP.


</div>


<p>

</section>


<section>

<h2 id="___sec14">Overhead present in parallel computing </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> <b>Uneven load balance</b>:  not all the processors can perform useful work at all time.</li>
<p><li> <b>Overhead of synchronization</b></li>
<p><li> <b>Overhead of communication</b></li>
<p><li> <b>Extra computation due to parallelization</b></li>
</ul>
<p>

Due to the above overhead and that certain part of a sequential
algorithm cannot be parallelized we may not achieve an optimal parallelization.
</div>


<p>

</section>


<section>

<h2 id="___sec15">Parallelizing a sequential algorithm  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Identify the part(s) of a sequential algorithm that can be  executed in parallel. This is the difficult part,</li>
<p><li> Distribute the global work and data among \( P \) processors.</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec16">Bindings to MPI routines  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
MPI is a message-passing library where all the routines
have corresponding C/C++-binding
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_COMMAND_NAME
</pre></div>
<p>
The discussion in these slides focuses on the C++ binding.


</div>


<p>

</section>


<section>

<h2 id="___sec17">Communicator  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> A group of MPI processes with a name (context).</li>
<p><li> Any process is identified by its rank. The rank is only meaningful within a particular communicator.</li>
<p><li> By default communicator \( MPI\_COMM\_WORLD \) contains all the MPI processes.</li>
<p><li> Mechanism to identify subset of processes.</li>
<p><li> Promotes modular design of parallel libraries.</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec18">Some of the most  important MPI functions </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> \( MPI\_Init \) - initiate an MPI computation</li>
<p><li> \( MPI\_Finalize \) - terminate the MPI computation and clean up</li>
<p><li> \( MPI\_Comm\_size \) - how many processes participate in a given MPI communicator?</li>
<p><li> \( MPI\_Comm\_rank \) - which one am I? (A number between 0 and size-1.)</li>
<p><li> \( MPI\_Send \) - send a message to a particular process within an MPI communicator</li>
<p><li> \( MPI\_Recv \) - receive a message from a particular process within an MPI communicator</li>
<p><li> \( MPI\_reduce \)  or \( MPI\_Allreduce \), send and receive messages</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec19">The first MPI C/C++ program  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Let every process write "Hello world" (oh not this program again!!) on the standard output. 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include &lt;mpi.h&gt;</span>
<span style="color: #1e889b">#include &lt;iostream&gt;</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank;
<span style="color: #228B22">//   MPI initializations</span>
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; <span style="color: #CD5555">&quot; out of &quot;</span> 
     &lt;&lt; numprocs &lt;&lt; endl;
<span style="color: #228B22">//  End MPI</span>
MPI_Finalize ();
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec20">The Fortran program </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=text (!bc forcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">PROGRAM hello
INCLUDE &quot;mpif.h&quot;
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)&quot;Hello world, I&#39;ve rank &quot;,my_rank,&quot; out of &quot;,size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec21">Note 1  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> The output to screen is not ordered since all processes are trying to write  to screen simultaneously.</li>
<p><li> It is the operating system which opts for an ordering.</li>  
<p><li> If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example.</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec22">Ordered output with \( MPI\_Barrier \)  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
 <span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank, i;
 MPI_Init (&amp;nargs, &amp;args);
 MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
 <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 <span style="color: #8B008B; font-weight: bold">if</span> (i == my_rank) {
 cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; 
        <span style="color: #CD5555">&quot; out of &quot;</span> &lt;&lt; numprocs &lt;&lt; endl;}
      MPI_Finalize ();
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec23">Note 2 </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> Here we have used the \( MPI\_Barrier \) function to ensure that that every process has completed  its set of instructions in  a particular order.</li>
<p><li> A barrier is a special collective operation that does not allow the processes to continue until all processes in the communicator (here \( MPI\_COMM\_WORLD \)) have called \( MPI\_Barrier \).</li> 
<p><li> The barriers make sure that all processes have reached the same point in the code. Many of the collective operations like \( MPI\_ALLREDUCE \) to be discussed later, have the same property; that is, no process can exit the operation until all processes have started.</li> 
</ul>
<p>

However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.


</div>


<p>

</section>


<section>

<h2 id="___sec24">Ordered output with \( MPI\_Recv \) and \( MPI\_Send \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=text (!bc ccpcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">.....
int numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
if (my_rank &gt; 0)
MPI_Recv (&amp;flag, 1, MPI_INT, my_rank-1, 100, 
           MPI_COMM_WORLD, &amp;status);
cout &lt;&lt; &quot;Hello world, I have  rank &quot; &lt;&lt; my_rank &lt;&lt; &quot; out of &quot; 
&lt;&lt; numprocs &lt;&lt; endl;
if (my_rank &lt; numprocs-1)
MPI_Send (&amp;my_rank, 1, MPI_INT, my_rank+1, 
          100, MPI_COMM_WORLD);
MPI_Finalize ();
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec25">Note 3  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
The basic sending of messages is given by the function \( MPI\_SEND \), which in C/C++
is defined as 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Send(<span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
             MPI_Datatype datatype, 
             <span style="color: #a7a7a7; font-weight: bold">int</span> dest, <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm)}
</pre></div>
<p>
This single command allows the passing of any kind of variable, even a large array, to any group of tasks. 
The variable <b>buf</b> is the variable we wish to send while <b>count</b>
is the  number of variables we are passing. If we are passing only a single value, this should be 1. 

<p>
If we transfer an array, it is  the overall size of the array. 
For example, if we want to send a 10 by 10 array, count would be \( 10\times 10=100 \) 
since we are  actually passing 100 values.  


</div>


<p>

</section>


<section>

<h2 id="___sec26">Note 4  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Once you have  sent a message, you must receive it on another task. The function \( MPI\_RECV \)
is similar to the send call.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Recv( <span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count, MPI_Datatype datatype, 
            <span style="color: #a7a7a7; font-weight: bold">int</span> source, 
            <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm, MPI_Status *status )
</pre></div>
<p>
The arguments that are different from those in MPI\_SEND are
<b>buf</b> which  is the name of the variable where you will  be storing the received data, 
<b>source</b> which  replaces the destination in the send command. This is the return ID of the sender.

<p>
Finally,  we have used  \( MPI\_Status\_status \),  
where one can check if the receive was completed.

<p>
The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.


</div>


<p>

</section>


<section>

<h2 id="___sec27">Numerical integration in parallel </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b>Integrating \( \pi \).</b>
<p>

<ul>
<p><li> The code example computes \( \pi \) using the trapezoidal rules.</li>
<p><li> The trapezoidal rule</li>
</ul>
<p>

<p>&nbsp;<br>
$$
   I=\int_a^bf(x) dx\approx h\left(f(a)/2 + f(a+h) +f(a+2h)+\dots +f(b-h)+ f(b)/2\right).
$$
<p>&nbsp;<br>



</div>


<p>

</section>


<section>

<h2 id="___sec28">Dissection of trapezoidal rule with \( MPI\_reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//    Trapezoidal rule and numerical integration usign MPI</span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include &lt;mpi.h&gt;</span>
<span style="color: #1e889b">#include &lt;iostream&gt;</span>

<span style="color: #228B22">//     Here we define various functions called by the main program</span>

<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> );
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">int</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> (*)(<span style="color: #a7a7a7; font-weight: bold">double</span>));

<span style="color: #228B22">//   Main function begins here</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> n, local_n, numprocs, my_rank; 
  <span style="color: #a7a7a7; font-weight: bold">double</span> a, b, h, local_a, local_b, total_sum, local_sum;   
  <span style="color: #a7a7a7; font-weight: bold">double</span>  time_start, time_end, total_time;
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec29">Dissection of trapezoidal rule with \( MPI\_reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">//  MPI initializations</span>
  MPI_Init (&amp;nargs, &amp;args);
  MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
  time_start = MPI_Wtime();
  <span style="color: #228B22">//  Fixed values for a, b and n </span>
  a = <span style="color: #B452CD">0.0</span> ; b = <span style="color: #B452CD">1.0</span>;  n = <span style="color: #B452CD">1000</span>;
  h = (b-a)/n;    <span style="color: #228B22">// h is the same for all processes </span>
  local_n = n/numprocs;  
  <span style="color: #228B22">// make sure n &gt; numprocs, else integer division gives zero</span>
  <span style="color: #228B22">// Length of each process&#39; interval of</span>
  <span style="color: #228B22">// integration = local_n*h.  </span>
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec30">Integrating with <b>MPI</b> </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  total_sum = <span style="color: #B452CD">0.0</span>;
  local_sum = trapezoidal_rule(local_a, local_b, local_n, 
                               &amp;int_function); 
  MPI_Reduce(&amp;local_sum, &amp;total_sum, <span style="color: #B452CD">1</span>, MPI_DOUBLE, 
              MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  <span style="color: #8B008B; font-weight: bold">if</span> ( my_rank == <span style="color: #B452CD">0</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Trapezoidal rule = &quot;</span> &lt;&lt;  total_sum &lt;&lt; endl;
    cout &lt;&lt; <span style="color: #CD5555">&quot;Time = &quot;</span> &lt;&lt;  total_time  
         &lt;&lt; <span style="color: #CD5555">&quot; on number of processors: &quot;</span>  &lt;&lt; numprocs  &lt;&lt; endl;
  }
  <span style="color: #228B22">// End MPI</span>
  MPI_Finalize ();  
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">// end of main program</span>
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec31">How do I use \( MPI\_reduce \)? </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Here we have used
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">MPI_reduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
     MPI_Datatype datatype, MPI_Op, <span style="color: #a7a7a7; font-weight: bold">int</span> root, MPI_Comm comm)
</pre></div>
<p>
The two variables \( senddata \) and \( resultdata \) are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size. 
The variable \( count \) represents the total dimensionality, 1 in case of just one variable, 
while \( MPI\_Datatype \) 
defines the type of variable which is sent and received.  

<p>
The new feature is \( MPI\_Op \). It defines the type
of operation we want to do.
</div>


<p>

</section>


<section>

<h2 id="___sec32">More on \( MPI\_Reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
In our case, since we are summing
the rectangle  contributions from every process we define  \( MPI\_Op = MPI\_SUM \).
If we have an array or matrix we can search for the largest og smallest element by sending either \( MPI\_MAX \) or 
\( MPI\_MIN \).  If we want the location as well (which array element) we simply transfer 
\( MPI\_MAXLOC \) or \( MPI\_MINOC \). If we want the product we write \( MPI\_PROD \). 

<p>
\( MPI\_Allreduce \) is defined as
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">MPI_Allreduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count, 
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)        
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec33">Dissection of trapezoidal rule with \( MPI\_reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
We use \( MPI\_reduce \) to collect data from each process. Note also the use of the function 
\( MPI\_Wtime \). 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the function to integrate</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> x)
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> value = <span style="color: #B452CD">4.</span>/(<span style="color: #B452CD">1.</span>+x*x);
  <span style="color: #8B008B; font-weight: bold">return</span> value;
} <span style="color: #228B22">// end of function to evaluate</span>
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec34">Dissection of trapezoidal rule with \( MPI\_reduce \) </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the trapezoidal rule</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">int</span> n, 
                         <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> trapez_sum;
  <span style="color: #a7a7a7; font-weight: bold">double</span> fa, fb, x, step;
  <span style="color: #a7a7a7; font-weight: bold">int</span>    j;
  step=(b-a)/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n);
  fa=(*func)(a)/<span style="color: #B452CD">2.</span> ;
  fb=(*func)(b)/<span style="color: #B452CD">2.</span> ;
  trapez_sum=<span style="color: #B452CD">0.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">1</span>; j &lt;= n-<span style="color: #B452CD">1</span>; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);
  }
  trapez_sum=(trapez_sum+fb+fa)*step;
  <span style="color: #8B008B; font-weight: bold">return</span> trapez_sum;
}  <span style="color: #228B22">// end trapezoidal_rule </span>
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec35">Optimization and profiling </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Till now we have not paid much attention to speed and possible optimization possibilities
inherent in the various compilers. We have compiled and linked as
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpic++  -c  mycode.cpp
mpic++  -o  mycode.exe  mycode.o
</pre></div>
<p>
For Fortran replace with mpif90.
This is what we call a flat compiler option and should be used when we develop the code.
It produces normally a very large and slow code when translated to machine instructions.
We use this option for debugging and for establishing the correct program output because
every operation is done precisely as the user specified it.

<p>
It is instructive to look up the compiler manual for further instructions
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">man mpic++  &gt;  out_to_file
</pre></div>

</div>


</section>


<section>

<h2 id="___sec36">More on optimization </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
We have additional compiler options for optimization. These may include procedure inlining where 
performance may be improved, moving constants inside loops outside the loop, 
identify potential parallelism, include automatic vectorization or replace a division with a reciprocal
and a multiplication if this speeds up the code.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpic++  -O3 -c  mycode.cpp
mpic++  -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
This is the recommended option. <b>But you must check that you get the same results as previously</b>.


</div>


</section>


<section>

<h2 id="___sec37">Optimization and profiling  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
It is also useful to profile your program under the development stage.
You would then compile with 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpic++  -pg -O3 -c  mycode.cpp
mpic++  -pg -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
After you have run the code you can obtain the profiling information via
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">gprof mycode.exe &gt;  out_to_profile
</pre></div>
<p>
When you have profiled properly your code, you must take out this option as it 
increases your CPU expenditure.
For memory tests use "valgrind":"http:www.valgrind.org". An excellent GUI is also Qt, with debugging facilities.


</div>


</section>


<section>

<h2 id="___sec38">Optimization and profiling  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Other hints

<ul>
<p><li> avoid if tests or call to functions inside loops, if possible.</li> 
<p><li> avoid multiplication with constants inside loops if possible</li>
</ul>
<p>

Bad code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">for</span> i = <span style="color: #B452CD">1</span>:n
    a(i) = b(i) +c*d
    e = g(k)
end
</pre></div>
<p>
Better code
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">temp = c*d
<span style="color: #8B008B; font-weight: bold">for</span> i = <span style="color: #B452CD">1</span>:n
    a(i) = b(i) + temp
end
e = g(k)
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec39">What is OpenMP </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> OpenMP provides high-level thread programming</li>
<p><li> Multiple cooperating threads are allowed to run simultaneously</li>
<p><li> Threads are created and destroyed dynamically in a fork-join pattern</li>

<ul>
   <p><li> An OpenMP program consists of a number of parallel regions</li>
   <p><li> Between two parallel regions there is only one master thread</li>
   <p><li> In the beginning of a parallel region, a team of new threads is spawned</li>
</ul>
<p>

  <p><li> The newly spawned threads work simultaneously with the master thread</li>
  <p><li> At the end of a parallel region, the new threads are destroyed</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec40">Getting started, things to remember </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <p><li> Remember the header file</li> 
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;omp.h&gt;</span>
</pre></div>
<ul>
 <p><li> Insert compiler directives in C++ syntax as</li> 
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp...</span>
</pre></div>
<ul>
<p><li> Compile with for example <em>c++ -fopenmp code.cpp</em></li>
<p><li> Execute</li>

<ul>
  <p><li> Remember to assign the environment variable <b>OMP NUM THREADS</b></li>
  <p><li> It specifies the total number of threads inside a parallel region, if not otherwise overwritten</li>
</ul>
<p>

</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec41">General code structure  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;omp.h&gt;</span>
main ()
{
<span style="color: #a7a7a7; font-weight: bold">int</span> var1, var2, var3;
<span style="color: #228B22">/* serial code */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #228B22">/* start of a parallel region */</span>
<span style="color: #1e889b">#pragma omp parallel private(var1, var2) shared(var3)</span>
{
<span style="color: #228B22">/* ... */</span>
}
<span style="color: #228B22">/* more serial code */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #228B22">/* another parallel region */</span>
<span style="color: #1e889b">#pragma omp parallel</span>
{
<span style="color: #228B22">/* ... */</span>
}
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec42">Parallel region </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> A parallel region is a block of code that is executed by a team of threads</li>
<p><li> The following compiler directive creates a parallel region</li>
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel { ... }</span>
</pre></div>
<ul>
<p><li> Clauses can be added at the end of the directive</li>
<p><li> Most often used clauses:</li>

<ul>
 <p><li> <b>default(shared)</b> or <b>default(none)</b></li>
 <p><li> <b>public(list of variables)</b></li>
 <p><li> <b>private(list of variables)</b></li>
</ul>
<p>

</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec43">Hello world, not again, please! </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;omp.h&gt;</span>
<span style="color: #1e889b">#include &lt;stdio.h&gt;</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span> *argv[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> th_id, nthreads;
<span style="color: #1e889b">#pragma omp parallel private(th_id) shared(nthreads)</span>
{
th_id = omp_get_thread_num();
printf(<span style="color: #CD5555">&quot;Hello World from thread %d\n&quot;</span>, th_id);
<span style="color: #1e889b">#pragma omp barrier</span>
<span style="color: #8B008B; font-weight: bold">if</span> ( th_id == <span style="color: #B452CD">0</span> ) {
nthreads = omp_get_num_threads();
printf(<span style="color: #CD5555">&quot;There are %d threads\n&quot;</span>,nthreads);
}
}
<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec44">Important OpenMP library routines </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> <b>int omp get num threads ()</b>, returns the number of threads inside a parallel region</li>
<p><li> <b>int omp get thread num ()</b>,  returns the  a thread for each thread inside a parallel region</li>
<p><li> <b>void omp set num threads (int)</b>, sets the number of threads to be used</li>
<p><li> <b>void omp set nested (int)</b>,  turns nested parallelism on/off</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec45">Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <p><li> Inside a parallel region, the following compiler directive can be used to parallelize a for-loop:</li>
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp for</span>
</pre></div>
<ul>
<p><li> Clauses can be added, such as</li>

<ul>
  <p><li> <b>schedule(static, chunk size)</b></li>
  <p><li> <b>schedule(dynamic, chunk size)</b></li> 
  <p><li> <b>schedule(guided, chunk size)</b> (non-deterministic allocation)</li>
  <p><li> <b>schedule(runtime)</b></li>
  <p><li> <b>private(list of variables)</b></li>
  <p><li> <b>reduction(operator:variable)</b></li>
  <p><li> <b>nowait</b></li>
</ul>
<p>

</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec46">Example code </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;omp.h&gt;</span>
<span style="color: #1e889b">#define CHUNKSIZE 100</span>
<span style="color: #1e889b">#define N</span>
<span style="color: #B452CD">1000</span>
<span style="color: #008b45">main</span> ()
{
<span style="color: #a7a7a7; font-weight: bold">int</span> i, chunk;
<span style="color: #a7a7a7; font-weight: bold">float</span> a[N], b[N], c[N];
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++)
a[i] = b[i] = i * <span style="color: #B452CD">1.0</span>;
chunk = CHUNKSIZE;
<span style="color: #1e889b">#pragma omp parallel shared(a,b,c,chunk) private(i)</span>
{
<span style="color: #1e889b">#pragma omp for schedule(dynamic,chunk)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; N; i++)
c[i] = a[i] + b[i];
} <span style="color: #228B22">/* end of parallel region */</span>
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec47">More on Parallel for loop </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> The number of loop iterations can not be non-deterministic; break, return, exit, goto not allowed inside the for-loop</li>
<p><li> The loop index is private to each thread</li>
<p><li> A reduction variable is special</li>

<ul>
  <p><li> During the for-loop there is a local private copy in each thread</li>
  <p><li> At the end of the for-loop, all the local copies are combined together by the reduction operation</li>
</ul>
<p>

<p><li> Unless the nowait clause is used, an implicit barrier synchronization will be added at the end by the compiler</li>
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// #pragma omp parallel and #pragma omp for</span>
</pre></div>
<p>
can be combined into
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel for</span>
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec48">Inner product </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>&nbsp;<br>
$$
\sum_{i=0}^{n-1} a_ib_i
$$
<p>&nbsp;<br>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> i;
<span style="color: #a7a7a7; font-weight: bold">double</span> sum = <span style="color: #B452CD">0.</span>;
<span style="color: #228B22">/* allocating and initializing arrays */</span>
<span style="color: #228B22">/* ... */</span>
<span style="color: #1e889b">#pragma omp parallel for default(shared) private(i) </span>
reduction(+:sum)
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;N; i++)
sum += a[i]*b[i];
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec49">Different threads do different tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
Different threads do different tasks independently, each section is executed by one thread.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel</span>
{
<span style="color: #1e889b">#pragma omp sections</span>
{
<span style="color: #1e889b">#pragma omp section</span>
funcA ();
<span style="color: #1e889b">#pragma omp section</span>
funcB ();
<span style="color: #1e889b">#pragma omp section</span>
funcC ();
}
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec50">Single execution  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp single { ... }</span>
</pre></div>
<p>
The code is executed by one thread only, no guarantee which thread

<p>
Can introduce an implicit barrier at the end
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp master { ... }</span>
</pre></div>
<p>
Code executed by the master thread, guaranteed and no implicit barrier at the end.
</div>


<p>

</section>


<section>

<h2 id="___sec51">Coordination and synchronization  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp barrier</span>
</pre></div>
<p>
Synchronization, must be encountered by all threads in a team (or none)
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp ordered { a block of codes }</span>
</pre></div>
<p>
is another form of synchronization (in sequential order).
The form
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp critical { a block of codes }</span>
</pre></div>
<p>
and 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp atomic { single assignment statement }</span>
</pre></div>
<p>
is  more efficient than 
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp critical</span>
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec52">Data scope  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> OpenMP data scope attribute clauses:</li>

<ul>
 <p><li> <b>shared</b></li>
 <p><li> <b>private</b></li>
 <p><li> <b>firstprivate</b></li>
 <p><li> <b>lastprivate</b></li>
 <p><li> <b>reduction</b></li>
</ul>
<p>

</ul>
<p>

What are the purposes of these attributes

<ul>
<p><li> define how and which variables are transferred to a parallel region (and back)</li>
<p><li> define which variables are visible to all threads in a parallel region, and which variables are privately allocated to each thread</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec53">Some remarks  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
<p><li> When entering a parallel region, the <b>private</b> clause ensures each thread having its own new variable instances. The new variables are assumed to be uninitialized.</li>
<p><li> A shared variable exists in only one memory location and all threads can read and write to that address. It is the programmer's responsibility to ensure that multiple threads properly access a shared variable.</li>
<p><li> The <b>firstprivate</b> clause combines the behavior of the private clause with automatic initialization.</li>
<p><li> The <b>lastprivate</b> clause combines the behavior of the private clause with a copy back (from the last loop iteration or section) to the original variable outside the parallel region.</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec54">Parallelizing nested for-loops </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<ul>
 <p><li> Serial code</li>
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;<span style="color: #B452CD">100</span>; i++)
<span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">0</span>; j&lt;<span style="color: #B452CD">100</span>; j++)
a[i][j] = b[i][j] + c[i][j]
</pre></div>
<ul>
<p><li> Parallelization</li>
</ul>
<p>

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel for private(j)</span>
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;<span style="color: #B452CD">100</span>; i++)
<span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">0</span>; j&lt;<span style="color: #B452CD">100</span>; j++)
a[i][j] = b[i][j] + c[i][j]
</pre></div>
<ul>
<p><li> Why not parallelize the inner loop? to save overhead of repeated thread forks-joins</li>
<p><li> Why must <b>j</b> be private? To avoid race condition among the threads</li>
</ul>
</div>


<p>

</section>


<section>

<h2 id="___sec55">Nested parallelism  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
When a thread in a parallel region encounters another parallel construct, it
may create a new team of threads and become the master of the new
team.
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel num_threads(4)</span>
{
<span style="color: #228B22">/* .... */</span>
<span style="color: #1e889b">#pragma omp parallel num_threads(2)</span>
{
<span style="color: #228B22">//  </span>
}
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec56">Parallel tasks </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp task </span>
<span style="color: #1e889b">#pragma omp parallel shared(p_vec) private(i)</span>
{
<span style="color: #1e889b">#pragma omp single</span>
{
<span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;N; i++) {
<span style="color: #a7a7a7; font-weight: bold">double</span> r = random_number();
<span style="color: #8B008B; font-weight: bold">if</span> (p_vec[i] &gt; r) {
<span style="color: #1e889b">#pragma omp task</span>
do_work (p_vec[i]);
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec57">Common mistakes </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
Race condition
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> nthreads;
<span style="color: #1e889b">#pragma omp parallel shared(nthreads)</span>
{
nthreads = omp_get_num_threads();
}
</pre></div>
<p>
Deadlock
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#pragma omp parallel</span>
{
...
<span style="color: #1e889b">#pragma omp critical</span>
{
...
<span style="color: #1e889b">#pragma omp barrier</span>
}
}
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec58">Matrix-matrix multiplication </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b"># include &lt;cstdlib&gt;</span>
<span style="color: #1e889b"># include &lt;iostream&gt;</span>
<span style="color: #1e889b"># include &lt;cmath&gt;</span>
<span style="color: #1e889b"># include &lt;ctime&gt;</span>
<span style="color: #1e889b"># include &lt;omp.h&gt;</span>

<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;

<span style="color: #228B22">// Main function</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> ( )
{
<span style="color: #228B22">// brute force coding of arrays</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> a[<span style="color: #B452CD">500</span>][<span style="color: #B452CD">500</span>];
  <span style="color: #a7a7a7; font-weight: bold">double</span> angle;
  <span style="color: #a7a7a7; font-weight: bold">double</span> b[<span style="color: #B452CD">500</span>][<span style="color: #B452CD">500</span>];
  <span style="color: #a7a7a7; font-weight: bold">double</span> c[<span style="color: #B452CD">500</span>][<span style="color: #B452CD">500</span>];
  <span style="color: #a7a7a7; font-weight: bold">int</span> i;
  <span style="color: #a7a7a7; font-weight: bold">int</span> j;
  <span style="color: #a7a7a7; font-weight: bold">int</span> k;
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec59">Matrix-matrix multiplication  </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #a7a7a7; font-weight: bold">int</span> n = <span style="color: #B452CD">500</span>;
  <span style="color: #a7a7a7; font-weight: bold">double</span> pi = acos(-<span style="color: #B452CD">1.0</span>);
  <span style="color: #a7a7a7; font-weight: bold">double</span> s;
  <span style="color: #a7a7a7; font-weight: bold">int</span> thread_num;
  <span style="color: #a7a7a7; font-weight: bold">double</span> wtime;

  cout &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;
  cout &lt;&lt; <span style="color: #CD5555">&quot;  C++/OpenMP version\n&quot;</span>;
  cout &lt;&lt; <span style="color: #CD5555">&quot;  Compute matrix product C = A * B.\n&quot;</span>;

  thread_num = omp_get_max_threads ( );

<span style="color: #228B22">//</span>
<span style="color: #228B22">//  Loop 1: Evaluate A.</span>
<span style="color: #228B22">//</span>
  s = <span style="color: #B452CD">1.0</span> / sqrt ( ( <span style="color: #a7a7a7; font-weight: bold">double</span> ) ( n ) );

  wtime = omp_get_wtime ( );
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec60">Matrix-matrix multiplication </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b"># pragma omp parallel shared ( a, b, c, n, pi, s ) </span>
<span style="color: #8B008B; font-weight: bold">private</span> ( angle, i, j, k )
{
  <span style="color: #a61717; background-color: #e3d2d2">#</span> pragma omp <span style="color: #8B008B; font-weight: bold">for</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( i = <span style="color: #B452CD">0</span>; i &lt; n; i++ )
  {
    <span style="color: #8B008B; font-weight: bold">for</span> ( j = <span style="color: #B452CD">0</span>; j &lt; n; j++ )
    {
      angle = <span style="color: #B452CD">2.0</span> * pi * i * j / ( <span style="color: #a7a7a7; font-weight: bold">double</span> ) n;
      a[i][j] = s * ( sin ( angle ) + cos ( angle ) );
    }
  }
<span style="color: #228B22">//</span>
<span style="color: #228B22">//  Loop 2: Copy A into B.</span>
<span style="color: #228B22">//</span>
  <span style="color: #a61717; background-color: #e3d2d2">#</span> pragma omp <span style="color: #8B008B; font-weight: bold">for</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( i = <span style="color: #B452CD">0</span>; i &lt; n; i++ )
  {
    <span style="color: #8B008B; font-weight: bold">for</span> ( j = <span style="color: #B452CD">0</span>; j &lt; n; j++ )
    {
      b[i][j] = a[i][j];
    }
  }
</pre></div>

</div>


<p>

</section>


<section>

<h2 id="___sec61">Matrix-matrix multiplication </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  Loop 3: Compute C = A * B.</span>
<span style="color: #228B22">//</span>
  <span style="color: #a61717; background-color: #e3d2d2">#</span> pragma omp <span style="color: #8B008B; font-weight: bold">for</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( i = <span style="color: #B452CD">0</span>; i &lt; n; i++ )
  {
    <span style="color: #8B008B; font-weight: bold">for</span> ( j = <span style="color: #B452CD">0</span>; j &lt; n; j++ )
    {
      c[i][j] = <span style="color: #B452CD">0.0</span>;
      <span style="color: #8B008B; font-weight: bold">for</span> ( k = <span style="color: #B452CD">0</span>; k &lt; n; k++ )
      {
        c[i][j] = c[i][j] + a[i][k] * b[k][j];
      }
    }
  }
}
  wtime = omp_get_wtime ( ) - wtime;
  cout &lt;&lt; <span style="color: #CD5555">&quot;  Elapsed seconds = &quot;</span> &lt;&lt; wtime &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;
  cout &lt;&lt; <span style="color: #CD5555">&quot;  C(100,100)  = &quot;</span> &lt;&lt; c[<span style="color: #B452CD">99</span>][<span style="color: #B452CD">99</span>] &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;
<span style="color: #228B22">//</span>
<span style="color: #228B22">//  Terminate.</span>
<span style="color: #228B22">//</span>
  cout &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;
  cout &lt;&lt; <span style="color: #CD5555">&quot;  Normal end of execution.\n&quot;</span>;
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
</pre></div>

</div>


<p>


</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.min.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1170,  // original: 960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>
