% Slides for fys4411

\documentclass[compress]{beamer}


% Try the class options [notes], [notes=only], [trans], [handout],
% [red], [compress], [draft], [class=article] and see what happens!

% For a green structure color use:
%\colorlet{structure}{green!50!black}

\mode<article> % only for the article version
{
  \usepackage{beamerbasearticle}
  \usepackage{fullpage}
  \usepackage{hyperref}
}

\beamertemplateshadingbackground{red!10}{blue!10}

%\usetheme{Hannover}

\setbeamertemplate{footline}[page number]


%\usepackage{beamerthemeshadow}



\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{shadow}
\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{basicstyle=\small}
%\lstset{backgroundcolor=\color{white}}
%\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
%\lstset{keywordstyle=\color{red}\bfseries}
%\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\usepackage{times}

% Use some nice templates
\beamertemplatetransparentcovereddynamic

% own commands

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
%\newcommand{\bra}[1]{\left\langle #1 \right|}
%\newcommand{\ket}[1]{\left| # \right\rangle}
\newcommand{\braket}[2]{\left\langle #1 \right| #2 \right\rangle}
\newcommand{\OP}[1]{{\bf\widehat{#1}}}
\newcommand{\matr}[1]{{\bf \cal{#1}}}
\newcommand{\beN}{\begin{equation*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\beaN}{\begin{eqnarray*}}
\newcommand{\eeN}{\end{equation*}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\eeaN}{\end{eqnarray*}}
\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
\newcommand{\bsubeqs}{\begin{subequations}}
\newcommand{\esubeqs}{\end{subequations}}
\def\psii{\psi_{i}}
\def\psij{\psi_{j}}
\def\psiij{\psi_{ij}}
\def\psisq{\psi^2}
\def\psisqex{\langle \psi^2 \rangle}
\def\psiR{\psi({\bf R})}
\def\psiRk{\psi({\bf R}_k)}
\def\psiiRk{\psi_{i}(\Rveck)}
\def\psijRk{\psi_{j}(\Rveck)}
\def\psiijRk{\psi_{ij}(\Rveck)}
\def\ranglep{\rangle_{\psisq}}
\def\Hpsibypsi{{H \psi \over \psi}}
\def\Hpsiibypsi{{H \psii \over \psi}}
\def\HmEpsibypsi{{(H-E) \psi \over \psi}}
\def\HmEpsiibypsi{{(H-E) \psii \over \psi}}
\def\HmEpsijbypsi{{(H-E) \psij \over \psi}}
\def\psiibypsi{{\psii \over \psi}}
\def\psijbypsi{{\psij \over \psi}}
\def\psiijbypsi{{\psiij \over \psi}}
\def\psiibypsiRk{{\psii(\Rveck) \over \psi(\Rveck)}}
\def\psijbypsiRk{{\psij(\Rveck) \over \psi(\Rveck)}}
\def\psiijbypsiRk{{\psiij(\Rveck) \over \psi(\Rveck)}}
\def\EL{E_{\rm L}}
\def\ELi{E_{{\rm L},i}}
\def\ELj{E_{{\rm L},j}}
\def\ELRk{E_{\rm L}(\Rveck)}
\def\ELiRk{E_{{\rm L},i}(\Rveck)}
\def\ELjRk{E_{{\rm L},j}(\Rveck)}
\def\Ebar{\bar{E}}
\def\Ei{\Ebar_{i}}
\def\Ej{\Ebar_{j}}
\def\Ebar{\bar{E}}
\def\Rvec{{\bf R}}
\def\Rveck{{\bf R}_k}
\def\Rvecl{{\bf R}_l}
\def\NMC{N_{\rm MC}}
\def\sumMC{\sum_{k=1}^{\NMC}}
\def\MC{Monte Carlo}
\def\adiag{a_{\rm diag}}
\def\tcorr{T_{\rm corr}}
\def\intR{{\int {\rm d}^{3N}\!\!R\;}}

\def\ul{\underline}
\def\beq{\begin{eqnarray}}
\def\eeq{\end{eqnarray}}

\newcommand{\eqbrace}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right.}
\newcommand{\eqbraced}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right\}}
\newcommand{\eqbracetriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right.}
\newcommand{\eqbracedtriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right\}}

\newcommand{\mybox}[3]{\mbox{\makebox[#1][#2]{$#3$}}}
\newcommand{\myframedbox}[3]{\mbox{\framebox[#1][#2]{$#3$}}}

%% Infinitesimal (and double infinitesimal), useful at end of integrals
%\newcommand{\ud}[1]{\mathrm d#1}
\newcommand{\ud}[1]{d#1}
\newcommand{\udd}[1]{d^2\!#1}

%% Operators, algebraic matrices, algebraic vectors

%% Operator (hat, bold or bold symbol, whichever you like best):
\newcommand{\op}[1]{\widehat{#1}}
%\newcommand{\op}[1]{\mathbf{#1}}
%\newcommand{\op}[1]{\boldsymbol{#1}}

%% Vector:
\renewcommand{\vec}[1]{\boldsymbol{#1}}

%% Matrix symbol:
%\newcommand{\matr}[1]{\boldsymbol{#1}}
%\newcommand{\bb}[1]{\mathbb{#1}}

%% Determinant symbol:
\renewcommand{\det}[1]{|#1|}

%% Means (expectation values) of varius sizes
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\meanb}[1]{\big\langle #1 \big\rangle}
\newcommand{\meanbb}[1]{\Big\langle #1 \Big\rangle}
\newcommand{\meanbbb}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\meanbbbb}[1]{\Bigg\langle #1 \Bigg\rangle}

%% Shorthands for text set in roman font
\newcommand{\prob}[0]{\mathrm{Prob}} %probability
\newcommand{\cov}[0]{\mathrm{Cov}}   %covariance
\newcommand{\var}[0]{\mathrm{Var}}   %variancd

%% Big-O (typically for specifying the speed scaling of an algorithm)
\newcommand{\bigO}{\mathcal{O}}

%% Real value of a complex number
\newcommand{\real}[1]{\mathrm{Re}\!\left\{#1\right\}}

%% Quantum mechanical state vectors and matrix elements (of different sizes)
%\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\bfv}[1]{\boldsymbol{#1}}                     % vector written as a boldface symbol
\newcommand{\Div}[1]{\nabla \bullet \vbf{#1}}           % define divergence
\newcommand{\Grad}[1]{\boldsymbol{\nabla}{#1}}

%%% DEFINITIOS FOR QUANTUM MECHANICS %%%
\newcommand{\Op}[1]{{\bf\widehat{#1}}}                    % define operator
\newcommand{\Obs}[1]{\langle{\Op{#1}\rangle}}             % define observable
\newcommand{\be}{\begin{equation}}                        % begin equation
\newcommand{\ee}{\end{equation}}                          % end equation
\newcommand{\PsiT}{\bfv{\Psi_T}(\bfv{R})}                       % symbol for trial wave function
\newcommand{\braket}[2]{\langle{#1}|\Op{#2}|{#1}\rangle}
\newcommand{\Det}[1]{{|\bfv{#1}|}}

\newcommand{\brab}[1]{\big\langle #1 \big|}
\newcommand{\brabb}[1]{\Big\langle #1 \Big|}
\newcommand{\brabbb}[1]{\bigg\langle #1 \bigg|}
\newcommand{\brabbbb}[1]{\Bigg\langle #1 \Bigg|}
%\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\ketb}[1]{\big| #1 \big\rangle}
\newcommand{\ketbb}[1]{\Big| #1 \Big\rangle}
\newcommand{\ketbbb}[1]{\bigg| #1 \bigg\rangle}
\newcommand{\ketbbbb}[1]{\Bigg| #1 \Bigg\rangle}
\newcommand{\overlap}[2]{\langle #1 | #2 \rangle}
\newcommand{\overlapb}[2]{\big\langle #1 \big| #2 \big\rangle}
\newcommand{\overlapbb}[2]{\Big\langle #1 \Big| #2 \Big\rangle}
\newcommand{\overlapbbb}[2]{\bigg\langle #1 \bigg| #2 \bigg\rangle}
\newcommand{\overlapbbbb}[2]{\Bigg\langle #1 \Bigg| #2 \Bigg\rangle}
\newcommand{\bracket}[3]{\langle #1 | #2 | #3 \rangle}
\newcommand{\bracketb}[3]{\big\langle #1 \big| #2 \big| #3 \big\rangle}
\newcommand{\bracketbb}[3]{\Big\langle #1 \Big| #2 \Big| #3 \Big\rangle}
\newcommand{\bracketbbb}[3]{\bigg\langle #1 \bigg| #2 \bigg| #3 \bigg\rangle}
\newcommand{\bracketbbbb}[3]{\Bigg\langle #1 \Bigg| #2 \Bigg| #3 \Bigg\rangle}
\newcommand{\projection}[2]
{| #1 \rangle \langle  #2 |}
\newcommand{\projectionb}[2]
{\big| #1 \big\rangle \big\langle #2 \big|}
\newcommand{\projectionbb}[2]
{ \Big| #1 \Big\rangle \Big\langle #2 \Big|}
\newcommand{\projectionbbb}[2]
{ \bigg| #1 \bigg\rangle \bigg\langle #2 \bigg|}
\newcommand{\projectionbbbb}[2]
{ \Bigg| #1 \Bigg\rangle \Bigg\langle #2 \Bigg|}


%% If you run out of greek symbols, here's another one you haven't
%% thought of:
\newcommand{\Feta}{\hspace{0.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[c]{1mm}{F}}}\end{turn}}
\newcommand{\feta}{\hspace{-1.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[b]{4mm}{f}}}\end{turn}}




\title[FYS4411]{Slides from FYS4411 Lectures}
\author[Computational Physics II]{%
  Morten Hjorth-Jensen}
\institute[ORNL, University of Oslo and MSU]{
  \inst{1}
  Department of Physics and Center of Mathematics for Applications\\
  University of Oslo, N-0316 Oslo, Norway}

  
\date[UiO]{Spring  2013}
\subject{Conjugate gradient methods}


\pgfdeclareimage[width=6cm,angle=270]{pi}{pi}
\pgfdeclareimage[width=4cm,angle=270]{gropp}{gropp}
\pgfdeclareimage[width=5cm,angle=270]{rothman}{rothman}
\pgfdeclareimage[width=4cm,angle=270]{thijssen}{thijssen}
\pgfdeclareimage[width=10cm,angle=270]{BEC_three_peaks}{BEC_three_peaks}

\begin{document}



\frame{\titlepage}



\frame
{
  \frametitle{Conjugate gradient (CG) method}
\begin{small}
{\scriptsize
The success of the CG method  for finding solutions of non-linear problems is based
on the theory of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
\[
  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}.
\]
In the iterative process we end up with a problem like
\[
  \hat{\bf {r}}= \hat{\bf {b}}-\hat{{\bf A}}\hat{\bf {x}},
\]
where $\hat{\bf {r}}$ is the so-called residual or error in the iterative process.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The residual is zero when we reach the minimum of the quadratic equation
\[
  P(\hat{\bf {x}})=\frac{1}{2}\hat{\bf {x}}^T\hat{{\bf A}}\hat{\bf {x}} - \hat{\bf {x}}^T\hat{\bf {b}},
\]
with the constraint that the matrix $\hat{{\bf A}}$ is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
$\hat{{\bf A}}$, which is called the Hessian, is given by the second-derivative of the variance.  This quantity is always positive definite. If we vary the energy, the Hessian may not always be positive definite. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
For the harmonic oscillator in one-dimension with a trial wave function and probability
\[
\psi_T(x) = e^{-\alpha^2 x^2} \qquad ,P_T(x)dx = \frac{e^{-2\alpha^2 x^2}dx}{\int dx e^{-2\alpha^2 x^2}}
\]
with $\alpha$ as the variational parameter. 
We have the following local energy
\[
E_L[\alpha] = \alpha^2+x^2\left(\frac{1}{2}-2\alpha^2\right),
\]
which results in the expectation value
\[
\langle  E_L[\alpha]\rangle = \frac{1}{2}\alpha^2+\frac{1}{8\alpha^2}
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
The derivative of the energy with respect to $\alpha$ gives 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
The condition 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = 0,
\]
gives the optimal $\alpha=1/\sqrt{2}$.
}
\end{small}
}






\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In general we end up computing the expectation value of the energy in terms 
of some parameters $\mathbf{\alpha}=\{\alpha_0,\alpha_1,\dots,\alpha_n\right})$
and we search for a minimum in parameter space.  
This leads to an energy minimization problem.

The elements of the gradient are ($Ei$ 
is the first derivative wrt to the variational parameter $\alpha_i$)
\beq
\Ei
&\!\!=&\!\! \left\langle \psiibypsi \EL + { H \psii \over \psi}
-2 \Ebar \psiibypsi \right\rangle
\label{first_deriv_nonherm}
\\
&\!\!=&\!\! 2\left\langle \psiibypsi (\EL - \Ebar) \right\rangle
\;\;\;\; {\rm (by\;Hermiticity)}.
\label{first_deriv}
\eeq
For our simple model we get the same expression for the first 
derivative (check it!).
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
Taking the second derivative the Hessian is
\beq
\Ebar_{ij}
&=& 2 \Bigg[
\left\langle \left( \psiijbypsi + {\psii\psij\over \psisq} \right) (\EL-\Ebar) \right\rangle \nonumber \\
&&
-\left\langle \psiibypsi \right\rangle \Ej
-\left\langle \psijbypsi \right\rangle \Ei
+ \left\langle \psiibypsi \ELj \right\rangle \Bigg].
\label{rappe}
\eeq
Note that many conjugate gradient approaches do need the Hessian (the functions to be discussed next week are examples of this)!
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We can also minimize the variance. In our simple model the variance is
\[
\sigma^2[\alpha] = \frac{1}{2}\alpha^4-\frac{1}{4}+\frac{1}{32\alpha^4},
\]
with first derivative 
\[
\frac{d \sigma^2[\alpha]}{d\alpha} = 2\alpha^3-\frac{1}{8\alpha^5}
\]
and a second derivative which is always positive 
\[
\frac{d^2\sigma^2[\alpha]}{d\alpha^2} = 6\alpha^2+\frac{5}{8\alpha^6}
\]
Some professional codes include both a variance and an energy variation.
}

\end{small}
}




\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
In our case $f$ can be either the energy or the variance.  If we choose the energy then we have 
\[
\hat{\bf {\alpha}}_{i+1}-\hat{\bf {\alpha}}_i=\hat{\bf {A}}^{-1}(\nabla E(\hat{\bf {\alpha}}_{i+1})-\nabla E(\hat{\bf {\alpha}}_i)).
\]
In the simple model gradient and the Hessian $\hat{\bf A}}$ are 
\[
\frac{d\langle  E_L[\alpha]\rangle}{d\alpha} = \alpha-\frac{1}{4\alpha^3}
\]
and a second derivative which is always positive (meaning that we find a minimum)
\[
\hat{\bf A}}= \frac{d^2\langle  E_L[\alpha]\rangle}{d\alpha^2} = 1+\frac{3}{4\alpha^4}
\]
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple example and demonstration}
\begin{small}
{\scriptsize
We get then
\[
\alpha_{i+1}=\frac{4}{3}\alpha_i-\frac{\alpha_i^4}{3\alpha_{i+1}^3},
\]
which can be rewritten as
\[
\alpha_{i+1}^4-\frac{4}{3}\alpha_i\alpha_{i+1}^4+\frac{1}{3}\alpha_i^4.
\]
}
\end{small}
}




\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
\begin{itemize}
\item Start your program with calling a function which implements for example the  CGM method %(function $dfpmin$).
\item This function needs the function for the expectation value of the local energy and
the derivative of the local energy.  
\item Your function $func$ is now the Metropolis part with a call to the local energy function.
For every call to the function $func$ many practitionser use 1000-10000 Monte Carlo cycles for the trial wave function.
\item This gives an expectation value for the energy which is returned by the function $func$.
\item When one calls the local energy one also computes the first derivative of the expectaction value of the local energy
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha}=
2\left\langle \psiibypsi (E_{L}[\alpha] - \langle E_{L}[\alpha] \rangle) \right\rangle.
\]
\end{itemize}
}
\end{small}
}



\frame
{
  \frametitle{Using the conjugate gradient method}
\begin{small}
{\scriptsize
The expectation value for the local energy of the Helium atom with a simple Slater determinant is given by
\[
\langle E_{L} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
When implementing the conjugate gradient method, uyou should test your numerical derivative with the derivative of the last expression, that is
\[
\frac{d\langle E_{L}[\alpha] \rangle}{d\alpha} = 2\alpha-2\left(Z-\frac{5}{16}\right).
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
In the CG method we define so-called conjugate directions and two vectors 
$\hat{\bf {s}}$ and $\hat{\bf {t}}$
are said to be
conjugate if 
\[
\hat{\bf {s}}^T\hat{{\bf A}}\hat{\bf {t}}= 0.
\]
The philosophy of the CG method is to perform searches in various conjugate directions
of our vectors $\hat{{\bf x}}_i$ obeying the above criterion, namely
\[
\hat{\bf {x}}_i^T\hat{{\bf A}}\hat{\bf {x}}_j= 0.
\]
Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if $\hat{\bf {s}}$ is conjugate to $\hat{\bf {t}}$, then $\hat{\bf {t}}$ is conjugate to $\hat{\bf {s}}$.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
An example is given by the eigenvectors of the matrix 
\[
\hat{\bf {v}}_i^T\hat{{\bf A}}\hat{\bf {v}}_j= \lambda\hat{\bf {v}}_i^T\hat{\bf {v}}_j,
\]
which is zero unless $i=j$. 

}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Assume now that we have a symmetric positive-definite matrix $\hat{\bf {A}}$ of size
$n\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector 
\[
\hat{\bf {x}}_{i+1}=\hat{\bf {x}}_{i}+\alpha_i\hat{\bf {p}}_{i}. 
\]
We assume that $\hat{\bf {p}}_{i}$ is a sequence of $n$ mutually conjugate directions. 
Then the $\hat{\bf {p}}_{i}$  form a basis of $R^n$ and we can expand the solution 
$  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}$ in this basis, namely
\[
  \hat{\bf {x}}  = \sum^{n}_{i=1} \alpha_i \hat{\bf {p}}_i.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
The coefficients are given by
\[
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\]
Multiplying with $\hat{\bf {p}}_k^T$  from the left gives
\[
  \hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {x}} = \sum^{n}_{i=1} \alpha_i\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {p}}_i= \hat{\bf {p}}_k^T \hat{\bf {b}},
\]
and we can define the coefficients $\alpha_k$ as 
\[
    \alpha_k = \frac{\hat{\bf {p}}_k^T \hat{\bf {b}}}{\hat{\bf {p}}_k^T \hat{\bf {A}} \hat{\bf {p}}_k}
\] 
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method and iterations}
\begin{small}
{\scriptsize
If we choose the conjugate vectors $\hat{\bf {p}}_k$ carefully, 
then we may not need all of them to obtain a good approximation to the solution 
$\hat{\bf {x}}$. 
So, we want to regard the conjugate gradient method as an iterative method. 
This also allows us to solve systems where $n$ is so large that the direct 
method would take too much time.

We denote the initial guess for $\hat{\bf {x}}$ as $\hat{\bf {x}}_0$. 
We can assume without loss of generality that 
\[
\hat{\bf {x}}_0=0,
\]
or consider the system 
\[
\hat{\bf {A}}\hat{\bf {z}} = \hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_0,
\]
instead.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Important, one can show that the solution $\hat{\bf {x}}$ is also the unique minimizer of the quadratic form
\[
  f(\hat{\bf {x}}) = \frac{1}{2}\hat{\bf {x}}^T\hat{\bf {A}}\hat{\bf {x}} - \hat{\bf {x}}^T \hat{\bf {x}} , \quad \hat{\bf {x}}\in\mathbf{R}^n. 
\]
This suggests taking the first basis vector $\hat{\bf {p}}_1$ 
to be the gradient of $f$ at $\hat{\bf {x}}=\hat{\bf {x}}_0$, 
which equals 
\[
\hat{\bf {A}}\hat{\bf {x}}_0-\hat{\bf {b}},
\]
and 
$\hat{\bf {x}}_0=0$ it is equal $-\hat{\bf {b}}$.
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.
}
\end{small}
}


\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
Let  $\hat{\bf {r}}_k$ be the residual at the $k$-th step:
\[
\hat{\bf {r}}_k=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k.
\]

Note that $\hat{\bf {r}}_k$ is the negative gradient of $f$ at 
$\hat{\bf {x}}=\hat{\bf {x}}_k$, 
so the gradient descent method would be to move in the direction $\hat{\bf {r}}_k$. 
Here, we insist that the directions $\hat{\bf {p}}_k$ are conjugate to each other, 
so we take the direction closest to the gradient $\hat{\bf {r}}_k$  
under the conjugacy constraint. 
This gives the following expression
\[
\hat{\bf {p}}_{k+1}=\hat{\bf {r}}_k-\frac{\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {r}}_k}{\hat{\bf {p}}_k^T\hat{\bf {A}}\hat{\bf {p}}_k} \hat{\bf {p}}_k.
\]
}
\end{small}
}

\frame
{
  \frametitle{Conjugate gradient method}
\begin{small}
{\scriptsize
We can also  compute the residual iteratively as
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_{k+1},
 \]
which equals
\[
\hat{\bf {b}}-\hat{\bf {A}}(\hat{\bf {x}}_k+\alpha_k\hat{\bf {p}}_k),
 \]
or
\[
(\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k)-\alpha_k\hat{\bf {A}}\hat{\bf {p}}_k,
 \]
which gives
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {r}}_k-\hat{\bf {A}}\hat{\bf {p}}_{k},
 \]
}
\end{small}
}



\frame
{
  \frametitle{Conjugate gradient method, our case}
\begin{small}
{\scriptsize
If we consider finding the minimum of a function $f$ using Newton's method,
that is search for a zero of the gradient of a function.  Near a point $x_i$
we have to second order
\[
f(\hat{\bf {x}})=f(\hat{\bf {x}}_i)+(\hat{\bf {x}}-\hat{\bf {x}}_i)\nabla f(\hat{\bf {x}}_i)
\frac{1}{2}(\hat{\bf {x}}-\hat{\bf {x}}_i)\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i)
\]
giving
\[
\nabla f(\hat{\bf {x}})=\nabla f(\hat{\bf {x}}_i)+\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i).
 \]
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
(here the exact result)
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]
}
\end{small}
}


\frame
{
  \frametitle{The simple model}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{pxy.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, contour plots}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{contours.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, the derivatives}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{derivatives.png}
      \end{figure}
}


\frame
{
  \frametitle{The simple model, steepest descent steps}
%\vspace{-3cm}
      \begin{figure}[htp]
        \includegraphics[width=0.8\textwidth]{directions.png}
      \end{figure}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
#include <cmath>
#include <iostream>
#include <fstream>
#include <iomanip>
#include "vectormatrixclass.h"
using namespace  std;
//   Main function begins here
int main(int  argc, char * argv[]){
  int dim = 2;
  Vector x(dim),xsd(dim), b(dim),x0(dim);
  Matrix A(dim,dim);
  
  // Set our initial guess
  x0(0) = x0(1) = 0;
  // Set the matrix  
  A(0,0) =  3;    A(1,0) =  2;   A(0,1) =  2;   A(1,1) =  6; 
  b(0) = 2; b(1) = -8;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  cout << "The Matrix A that we are using: " << endl;
  A.Print();
  cout << endl;


  x = ConjugateGradient(A,b,x0);

  xsd = SteepestDescent(A,b,x0);
  
  cout << "The approximate solution using Conjugate Gradient is: " << endl;
  x.Print();
  cout << endl;

  cout << "The approximate solution using Steepest Descent is: " << endl;
  xsd.Print();
  cout << endl;
}
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
Vector SteepestDescent(Matrix A, Vector b, Vector x0){
  int IterMax, i;
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),f(dim),z(dim);
  double c,alpha,d;
  IterMax = 30;
  x = x0;
  f = A*x-b;
  i = 0;
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  while (i <= IterMax){
    z = A*f;
    c = dot(f,f);
    alpha = c/dot(f,z);
    x = x - alpha*f;
    f =  A*x-b;
    if(sqrt(dot(f,f)) < tolerance) break;
    i++;
  }
  return x;
} 
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  Vector ConjugateGradient(Matrix A, Vector b, Vector x0){
  int dim = x0.Dimension();
  const double tolerance = 1.0e-14;
  Vector x(dim),r(dim),v(dim),z(dim);
  double c,t,d;

  x = x0;
  r = b - A*x;
  v = r;
  c = dot(r,r);
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Simple codes for  steepest descent and conjugate gradient }
\begin{small}
{\scriptsize
\begin{verbatim}
  int i = 0; IterMax = dim;
  while(i <= IterMax){
    z = A*v;
    t = c/dot(v,z);
    x = x + t*v;
    r = r - t*z;
    d = dot(r,r);
    if(sqrt(d) < tolerance)
      break;
    v = r + (d/c)*v;
    c = d;  i++;
  }
  return x;
} 
\end{verbatim}
}
\end{small}
}






\frame[containsverbatim]
{
  \frametitle{Codes from numerical recipes}
\begin{small}
{\scriptsize
The codes are taken from chapter 10.7 of Numerical recipes.  We use the functions
$dfpmin$ and $lnsrch$.  You can load down the package of programs from the webpage of
the course, see under project 1.  
The package is called $NRcgm107.tar.gz$ and contains the files 
$dfmin.c$, $lnsrch.c$, $nrutil.c$ and $nrutil.h$. 
These codes are  written in C. 
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{What you have to provide}
\begin{small}
{\scriptsize
The input to $dfpmin$
\begin{verbatim}

void dfpmin(double p[], int n, double gtol, int *iter, double *fret,
double(*func)(double []), void (*dfunc)(double [], double []))

\end{verbatim}
is
\begin{itemize}
\item The starting vector $p$ of length $n$
\item The function $func$ on which minimization is done
\item The function $dfunc$ where the gradient i calculated
\item The convergence requirement for zeroing the gradient $gtol$.
\end{itemize}
It returns in $p$ the location of the minimum, the number of iterations and 
the minimum value of the function under study $fret$.
}
\end{small}
}










\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
#include "nrutil.h"
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{
  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);
  return value;
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{
  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
using namespace std;
//     Here we define various functions called by the main program

double E_function(double *x);
void   dE_function(double *x, double *g);
void   dfpmin(double p[], int n, double gtol, int *iter, double *fret,
	    double(*func)(double []), void (*dfunc)(double [], double []));
//   Main function begins here
int main()
{
     int n, iter;
     double gtol, fret;
     double alpha;
     n = 1;
     cout << "Read in guess for alpha" << endl;
     cin >> alpha;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//   reserve space in memory for vectors containing the variational
//   parameters
     double *p = new double [2];
     gtol = 1.0e-5;
//   now call dfmin and compute the minimum
     p[1] = alpha;
     dfpmin(p, n, gtol, &iter, &fret,&E_function,&dE_function);
     cout << "Value of energy minimum = " << fret << endl;
     cout << "Number of iterations = " << iter << endl;
     cout << "Value of alpha at minimu = " << p[1] << endl;
      delete [] p;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the Energy function
double E_function(double x[])
{

//  Change here by calling your Metropolis function which 
//  returns the local energy

  double value = x[1]*x[1]*0.5+1.0/(8*x[1]*x[1]);



  return value;
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Simple example and code (model.cpp on webpage)}
\begin{small}
{\scriptsize
\begin{verbatim}
//  this function defines the derivative of the energy 
void dE_function(double x[], double g[])
{

//  Change here by calling your Metropolis function. 
//  I compute both the local energy and its derivative for every call to func

  g[1] = x[1]-1.0/(4*x[1]*x[1]*x[1]);
} // end of function to evaluate
\end{verbatim}
You need to change this function so that you call the local energy for your system. I used 1000
cycles per call to get a new value of $\langle E_L[\alpha]\rangle$.
When I compute the local energy I also compute its derivative.
After roughly 10-20 iterations I got a converged result in terms of $\alpha$.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
The 
H$_2$ molecule consists of two protons and two electrons 
with a ground state energy $E=-31.949$ eV or $-1.175$ a.u. and the 
equilibrium distance between the two hydrogen atoms
of $r_0=1.40$ Bohr radii (recall that a Bohr radius is $0.05\times 10^{-9}$m.


We define our systems using the following variables.
Origo is chosen to be halfway between the two protons. The distance from 
proton 1 is defined as 
$-{\bf R}/2$ whereas proton 2 has a distance ${\bf R}/2$.
Calculations are performed for fixed distances ${\bf R}$ between the two protons.

}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
Electron 1 has a distance $r_1$ from the chose origo, while  electron $2$
has a distance $r_2$. 
The kinetic energy operator becomes then
\[
   -\frac{\nabla_1^2}{2}-\frac{\nabla_2^2}{2}.
\]
The distance between the two electrons is
$r_{12}=|{\bf r}_1-{\bf r}_2|$. 
The repulsion between the two electrons results in a potential energy term given by
\[
               +\frac{1}{r_{12}}.
\]
In a similar way we obtain a repulsive contribution from the interaction between the two 
protons given by
\[
               +\frac{1}{|{\bf R}|},
\]
where ${\bf R}$ is the distance between the two protons.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
To obtain the final potential energy we need to include the attraction the electrons feel from the protons.
To model this, we need to define the distance between the electrons and the two protons.
If we model this along a 
chosen $z$-akse with electron 1 placed at a distance 
${\bf r}_1$ from a chose origo, one proton at $-{\bf R}/2$
and the other at  ${\bf R}/2$, 
the distance from proton 1 to electron 1 becomes
\[
{\bf r}_{1p1}={\bf r}_1+ {\bf R}/2,
\]
and
\[
{\bf r}_{1p2}={\bf r}_1- {\bf R}/2,
\]
from proton 2.
}
\end{small}
}


\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
Similarly, for electron 2 we obtain
\[
{\bf r}_{2p1}={\bf r}_2+{\bf R}/2,
\]
and
\[
{\bf r}_{2p2}={\bf r}_2-{\bf R}/2.
\]
These four distances define the attractive contributions to the potential energy
\[
   -\frac{1}{r_{1p1}}-\frac{1}{r_{1p2}}-\frac{1}{r_{2p1}}-\frac{1}{r_{2p2}}.
\]
We can then write the total Hamiltonian as 
\[
   \OP{H}=-\frac{\nabla_1^2}{2}-\frac{\nabla_2^2}{2}
   -\frac{1}{r_{1p1}}-\frac{1}{r_{1p2}}-\frac{1}{r_{2p1}}-\frac{1}{r_{2p2}}
               +\frac{1}{r_{12}}+\frac{1}{|{\bf R}|}.
\]
}
\end{small}
}
\frame
{
  \frametitle{The hydrogen molecule, our next step}
\begin{small}
{\scriptsize
We will use a trial wave function of the form
\[
   \psi_{T}({\bf r_1},{\bf r_2}, {\bf R}) =
   \psi({\bf r}_1,{\bf R})\psi({\bf r}_2,{\bf R})
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)},
\]
with the following trial wave function 
\[
   \psi({\bf r}_1,{\bf R})=\left(\exp{(-\alpha r_{1p1})}
      +\exp{(-\alpha r_{1p2})}\right),
\]
for electron 1 and
\[
   \psi({\bf r}_2,{\bf R})=\left(\exp{(-\alpha r_{2p1})}
      +\exp{(-\alpha r_{2p2})}\right).
\]
The variational parameters are $\alpha$ and $\beta$.

}
\end{small}
}
\frame
{
  \frametitle{The Be$_2$ molecule, our final step}
\begin{small}
{\scriptsize
Our final step consists in estimating the binding energy of the Be$_2$ molecule.
Useful references are then
\begin{enumerate}
\item Moskowitz and Kalos, Int.~Journal of Quantum Chemistry {\bf XX}, 1107 (1981).
Results for He and H$_2$.
\item Filippi, Singh and Umrigar, J.~Chemical Physics {\bf 105}, 123 (1996).   Useful results on
Be$_2$ to which you can benchmark your results against.
\end{enumerate}
}
\end{small}
}

\end{document}


